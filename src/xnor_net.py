""" Class and method definition for the layers in XNOR-Net
"""
import theano
import numpy as np
import lasagne
import theano.tensor as T
import time

class BinaryActivLayer(lasagne.layers.Layer):
    """ Binary activation layer as described in XNOR-Net paper.
    This computes the scaling matrix K and the binary input I. Same notaions
    followed.
    """
    def __init__(self, incoming):


class BinaryConvLayer(lasagne.layers.Conv2DLayer):
    """ Binary convolution layer which performs convolution using XNOR and popcount operations.
    This is followed by the scaling with input and weight scaling factors K and alpha respectively.
    """

    def __init__(self, incoming, num_filters, filter_size, **kwargs):
        """
        Parameters
        -----------
        incoming : layer or tuple
            Ipnut layer to this layer. If this is fed by a data layer then this is a tuple representing input dimensions.
        num_filters: int
            Number of 3D filters present in this layer = No of feature maps generated by this layer
        filter_size: tuple
            Filter size of this layer. Leading dimension is = no of input feature maps.
        """
    
        # Matrix containing scaling parameters(alpha) for weight matrices
        # A[i] --> alpha for ith filter, Bi
        self.A = np.zeros(num_filters, dtype=np.float32)

        super(BinaryConvLayer, self).__init__(incoming,
            num_filters, filter_size, b=None, nonlinearity=None, flip_filters=False, **kwargs)

    def convolve(self, input, **kwargs):
        """ Binary convolution. Both inputs and weights are binary (+1 or -1)
        This overrides convolve operation from Conv2DLayer implementation
        """
        # compute the binary inputs H and the scaling matrix K
        self.H = np.sign(input)
        # np.sign(0) = 0, but we want that to be +1
        self.H[np.where(self.H == 0.)] = 1.0 

        # compute input scaling factor matrix K for every input in the batch
        # find sum of the absolute values across all channels for each input in the batch
        assert(len(input.shape) == 4)
        A = np.absolute(input)
        # mean across the channels/feature maps
        A = np.mean(A, axis=1)
        # average filters for the computation of K
        k = np.ones(shape=(self.W.shape[2], self.W.shape[3])) / (self.W.shape[2]*self.W.shape[3])

        # K will have scaling matrixces for each input in the batch.
        # K's shape = (batch_size, map_height, map_width)
        self.K = theano.tensor.signal.conv.conv2d(A, k) 

        # Compute the binarized filters are the scaling matrix
        self.B = np.sign(self.W)
        self.B[np.where(self.B == 0.)] = 1.0
        self.alpha = np.absolute(self.W).reshape(-1, self.W.shape[1]*self.W.shape[2]*self.W.shape[3])
        self.alpha = np.mean(alpha, axis=1)

        # TODO: Use XNOR ops for the convolution. As of now using Lasagne's convolution for
        # functionality verification.
        W_full_precision = self.W
        self.W = self.B
        feat_maps = super(BinaryConvLayer, self).convolve(self.H, **kwargs)
        self.W = W_full_precision

        # scale by K and alpha
        for b in range(feat_maps.shape[0]):
            feat_maps[b] = feat_maps[b] * self.K[b]
        for f in range(self.num_filters):
            feat_maps[:,f, :, :] = feat_maps[:, f, :, :] * self.alpha[f]

        return feat_maps

class BinaryDenseLayer(lasagne.layers.DenseLayer):
    """Binary version of fully connected layer. XNOR and bitcount ops are used for 
    this in a similar fashion as that of Conv Layer.
    """

    

def xnor_net_find_gradiants(net, loss):
    """ Method to compute gradiants of all layers for back propagation.
    This is similar to the method in BinaryNet implementation.
    """
