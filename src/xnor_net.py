""" Class and method definition for the layers in XNOR-Net
"""
import theano
import numpy as np
import lasagne
import theano.tensor as T
import time

#TODO: The trainable parameters and computations should be converted from numpy operation into theano tensor symbolic operations.
class BinaryActivLayer(lasagne.layers.Layer):
    """ Binary activation layer as described in XNOR-Net paper.
    This computes the scaling matrix K and the binary input I. Same notaions
    followed.
    """
    def __init__(self, incoming):


class BinaryConvLayer(lasagne.layers.Conv2DLayer):
    """ Binary convolution layer which performs convolution using XNOR and popcount operations.
    This is followed by the scaling with input and weight scaling factors K and alpha respectively.
    """

    def __init__(self, incoming, num_filters, filter_size, **kwargs):
        """
        Parameters
        -----------
        incoming : layer or tuple
            Ipnut layer to this layer. If this is fed by a data layer then this is a tuple representing input dimensions.
        num_filters: int
            Number of 3D filters present in this layer = No of feature maps generated by this layer
        filter_size: tuple
            Filter size of this layer. Leading dimension is = no of input feature maps.
        """
    
        # Matrix containing scaling parameters(alpha) for weight matrices
        # alpha[i] --> alpha for ith filter, Bi
        self.alpha = np.zeros(num_filters, dtype=np.float32)
        # paper does not talk about how to handle convolution biases.
        # hence neglecting biases
        super(BinaryConvLayer, self).__init__(incoming,
            num_filters, filter_size, b=None, nonlinearity=None, flip_filters=False, **kwargs)

    def convolve(self, input, **kwargs):
        """ Binary convolution. Both inputs and weights are binary (+1 or -1)
        This overrides convolve operation from Conv2DLayer implementation
        """
        # compute the binary inputs H and the scaling matrix K
        self.H = np.sign(input)
        # np.sign(0) = 0, but we want that to be +1
        self.H[np.where(self.H == 0.)] = 1.0 

        # compute input scaling factor matrix K for every input in the batch
        # find sum of the absolute values across all channels for each input in the batch
        assert(input.ndim == 4)
        A = np.absolute(input)
        # mean across the channels/feature maps
        A = np.mean(A, axis=1)
        # average filters for the computation of K
        k = np.ones(shape=(self.W.shape[2], self.W.shape[3])) / (self.W.shape[2]*self.W.shape[3])

        # K will have scaling matrixces for each input in the batch.
        # K's shape = (batch_size, map_height, map_width)
        self.K = theano.tensor.signal.conv.conv2d(A, k) 

        # Compute the binarized filters are the scaling matrix
        self.B = np.sign(self.W)
        self.B[np.where(self.B == 0.)] = 1.0
        self.alpha = np.absolute(self.W).reshape(-1, self.W.shape[1]*self.W.shape[2]*self.W.shape[3])
        self.alpha = np.mean(alpha, axis=1)

        # TODO: Use XNOR ops for the convolution. As of now using Lasagne's convolution for
        # functionality verification.
        # approx weight tensor
        W_full_precision = self.B * self.alpha[:, np.newaxis, np.newaxis, np.newaxis] 
        self.W = self.B
        feat_maps = super(BinaryConvLayer, self).convolve(self.H, **kwargs)
        # restore the approx full precision weight for gradiant computation
        self.W = W_full_precision

        # scale by K and alpha
        feat_maps = feat_maps * self.K[:,np.newaxis, :, :]

        feat_maps = feat_maps * self.alpha[:, np.newaxis, np.newaxis, np.newaxis]

        return feat_maps

class BinaryDenseLayer(lasagne.layers.DenseLayer):
    """Binary version of fully connected layer. XNOR and bitcount ops are used for 
    this in a similar fashion as that of Conv Layer.
    """

    def __init__(self, incoming, num_units, **kwargs):
       """
       """
       super(BinaryDenseLayer, self)__init__(incoming, num_units)

       # scaling factor for the binary weights
       self.alpha = np.zeros(num_units)
      

    def get_output_for(self, input, **kwargs):
        """ Binary dense layer dot product computation
        """
        # binarize the input
        self.H = np.sign(input)
        # np.sign(0) = 0, but we want that to be +1
        self.H[np.where(self.H == 0.)] = 1.0

        # compute input scaling factor for all inputs in the batch. assuming input is a theano variable
        self.beta = np.mean(np.absolute(input).flatten(2) axis=1)

        # compute weight scaling factor.

        # find the dot product
        # scale the output by alpha and beta

def xnor_net_find_gradiants(net, loss):
    """ Method to compute gradiants of all layers for back propagation.
    This is similar to the method in BinaryNet implementation.
    """
    NotImplementedError
