""" Class and method definition for the layers in XNOR-Net
"""
import theano
import theano.tensor.nnet
import numpy as np
import lasagne
import theano.tensor as T
import time
from fxp_helper import to_fixed_point_theano

def SignTheano(x):
    return T.cast(2.*T.ge(x,0)-1., theano.config.floatX)

def SignNumpy(x):
    return np.float32(2.*np.greater_equal(x,0)-1.)

def binarize_conv_input(conv_input, k):

    bin_conv_out = SignTheano(conv_input)

    # scaling factor for the activation.
    A =T.abs_(conv_input)

    # K will have scaling matrixces for each input in the batch.
    # K's shape = (batch_size, 1, map_height, map_width)
    k_shape = k.eval().shape
    pad = (k_shape[-2]/2, k_shape[-1]/2)
    K = theano.tensor.nnet.conv2d(A, k, border_mode=pad)

    return bin_conv_out, K
    

def binarize_fc_input(fc_input):

    bin_out = SignTheano(fc_input)
    
    if(fc_input.ndim == 4):  # prev layer is conv or pooling. hence compute the l1 norm using all maps
        beta = T.mean(T.abs_(fc_input), axis=[1, 2, 3])

    else: # feeding layer is FC layer
        beta = T.mean(T.abs_(fc_input), axis=1)

    return bin_out, beta



class Conv2DLayer(lasagne.layers.Conv2DLayer):
    """ Binary convolution layer which performs convolution using XNOR and popcount operations.
    This is followed by the scaling with input and weight scaling factors K and alpha respectively.
    """

    def __init__(self, incoming, num_filters, filter_size, format='float', data_bits=15, int_bits=0, **kwargs):

        """
        Parameters
        -----------
        incoming : layer or tuple
            Ipnut layer to this layer. If this is fed by a data layer then this is a tuple representing input dimensions.
        num_filters: int
            Number of 3D filters present in this layer = No of feature maps generated by this layer
        filter_size: tuple
            Filter size of this layer. Leading dimension is = no of input feature maps.
        """

        # average filter to compute scaling factor for activation
        no_inputs = incoming.output_shape[1]
        shape = (num_filters, no_inputs, filter_size[0], filter_size[1])

        super(Conv2DLayer, self).__init__(incoming, num_filters, filter_size, **kwargs)

        # average filter to compute the activation scaling factor
        beta_filter = np.ones(shape=shape).astype(np.float32) / (no_inputs*filter_size[0]*filter_size[1])
        self.beta_filter = self.add_param(beta_filter, shape, name='beta_filter', trainable=False, regularizable=False)

        # binary weight scaling factor.
        xalpha = lasagne.init.Constant(0.1)            
        self.xalpha = self.add_param(xalpha, [num_filters,], name='xalpha', trainable=False, regularizable=False)

        # params for fixed point simulation
        self.format = format
        self.data_bits = theano.shared(data_bits)
        self.int_bits = theano.shared(int_bits)

    def convolve(self, input, deterministic=False, **kwargs):
        """ Binary convolution. Both inputs and weights are binary (+1 or -1)
        This overrides convolve operation from Conv2DLayer implementation
        """
        if(self.format == 'fixed'):
            # reduce precision of input as this influences computation of beta
            fxp_input = to_fixed_point_theano(input, self.data_bits, self.int_bits)

            # compute the binary inputs H and the scaling matrix K
            bin_input, K = binarize_conv_input(fxp_input, self.beta_filter)
            # self.W must be already binarized during model init. Here we just call the parents conv method
            feat_maps = super(Conv2DLayer, self).convolve(bin_input, **kwargs)

            feat_maps = feat_maps * K
            feat_maps = feat_maps * self.xalpha.dimshuffle('x', 0, 'x', 'x')
            feat_maps = to_fixed_point_theano(feat_maps, self.data_bits, self.int_bits)
        else:
            # compute the binary inputs H and the scaling matrix K
            input, K = binarize_conv_input(input, self.beta_filter)

            # self.W must be already binarized during model init. Here we just call the parents conv method
            feat_maps = super(Conv2DLayer, self).convolve(input, **kwargs)

            # scale by K and alpha
            # FIXME: Actually we are scaling after adding bias here. Need to scale first and then add bias.
            # The super class method automatically adds bias. Somehow need to overcome this..
            # may subtract the bias, scale by alpha and beta ans then add bias ?
            feat_maps = feat_maps * K

            feat_maps = feat_maps * self.xalpha.dimshuffle('x', 0, 'x', 'x')
    
        return feat_maps

class DenseLayer(lasagne.layers.DenseLayer):
    """Binary version of fully connected layer. XNOR and bitcount ops are used for 
    this in a similar fashion as that of Conv Layer.
    """

    def __init__(self, incoming, num_units, format='float', data_bits=15, int_bits=0, **kwargs):
        """ XNOR-Net fully connected layer
        """
        num_inputs = int(np.prod(incoming.output_shape[1:]))
        super(DenseLayer, self).__init__(incoming, num_units,  **kwargs)

        # xnor-net dense layer requires one more new parameter for weight scaling.
        xalpha = np.zeros(shape=(num_units,), dtype=np.float32)
        self.xalpha = self.add_param(xalpha, xalpha.shape, name='xalpha', trainable=False, regularizable=False)

        # params for fixed point simulation
        self.format = format
        self.data_bits = theano.shared(data_bits)
        self.int_bits = theano.shared(int_bits)

    def get_output_for(self, input, deterministic=True, **kwargs):
        """ Binary dense layer dot product computation
        """
        if(self.format == 'fixed'):
            # reduce precision of input as this influences computation of beta
            fxp_input = to_fixed_point_theano(input, self.data_bits, self.int_bits)
            bin_input, beta = binarize_fc_input(fxp_input)
            # beta will have same range as input because it is just L1 norm of input
            # dot-product at full precision
            fc_out = super(DenseLayer, self).get_output_for(bin_input, **kwargs)
            
            fc_out = fc_out * beta.dimshuffle(0, 'x')

            fc_out = fc_out * self.xalpha.dimshuffle('x', 0)
            # reduce the precision of the output based on the data bit widths specified/
            fc_out = to_fixed_point_theano(fc_out, self.data_bits, self.int_bits)
        else:
            # binarize the input
            bin_input, beta = binarize_fc_input(input)

                
            fc_out = super(DenseLayer, self).get_output_for(bin_input, **kwargs)
            # scale the output by alpha and beta
            # FIXME: Actually we are scaling after adding bias here. Need to scale first and then add bias.
            # The super class method automatically adds bias. Somehow need to overcome this..
            # may subtract the bias, scale by alpha and beta ans then add bias ?
            fc_out = fc_out * beta.dimshuffle(0, 'x')

            fc_out = fc_out * self.xalpha.dimshuffle('x', 0)
            
        return fc_out


